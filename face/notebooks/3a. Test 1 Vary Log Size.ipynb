{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ipynb.fs.full.GenerateLogs import generate_log, get_data, get_encoding_graph, do_chinese_whispers, estimation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acute-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 24517 images of 6743 different people \n",
      "\n",
      "Log of length 1000 with 50 unique faces\n",
      "DBScan: people: 50 clusters: 52 error: 4.000000\n",
      "CW:     people: 50 clusters: 54 error: 8.000000\n",
      "DBscan took 0.0253 seconds\n",
      "CW     took 0.1555 seconds\n",
      "----------------------------------------------\n",
      "Log of length 1000 with 100 unique faces\n",
      "DBScan: people: 100 clusters: 103 error: 3.000000\n",
      "CW:     people: 100 clusters: 105 error: 5.000000\n",
      "DBscan took 0.0306 seconds\n",
      "CW     took 0.2168 seconds\n",
      "----------------------------------------------\n",
      "Log of length 1000 with 250 unique faces\n",
      "DBScan: people: 250 clusters: 259 error: 3.600000\n",
      "CW:     people: 250 clusters: 238 error: 4.800000\n",
      "DBscan took 0.0371 seconds\n",
      "CW     took 0.1135 seconds\n",
      "----------------------------------------------\n",
      "Log of length 1000 with 400 unique faces\n",
      "DBScan: people: 400 clusters: 377 error: 5.750000\n",
      "CW:     people: 400 clusters: 369 error: 7.750000\n",
      "DBscan took 0.0281 seconds\n",
      "CW     took 0.1044 seconds\n",
      "----------------------------------------------\n",
      "Log of length 1000 with 500 unique faces\n",
      "DBScan: people: 500 clusters: 466 error: 6.800000\n",
      "CW:     people: 500 clusters: 444 error: 11.200000\n",
      "DBscan took 0.0260 seconds\n",
      "CW     took 0.1219 seconds\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fix log_length, vary n_people\n",
    "log_length = 1000\n",
    "n_peoples = [50, 100, 250, 400, 500]\n",
    "\n",
    "epsilon = 9.8\n",
    "threshold = 72\n",
    "\n",
    "# Get data\n",
    "data_df = get_data(['lfw', 'cf'])\n",
    "\n",
    "for n_people in n_peoples:\n",
    "    # Generate log\n",
    "    log = generate_log(data_df, log_length=log_length, n_faces=n_people, exact=True).sample(frac=1).reset_index(drop=True)\n",
    "    n_people = len(log['id'].value_counts().keys())\n",
    "    print('Log of length %d with %d unique faces' % (len(log), n_people))\n",
    "        \n",
    "    # DBScan\n",
    "    X = log.drop(columns='id').values\n",
    "    # Do DBScan - Timed\n",
    "    db_start = time.perf_counter()\n",
    "    db = DBSCAN(eps=epsilon, min_samples=1).fit(X)\n",
    "    db_end = time.perf_counter()\n",
    "    labels = pd.Series(db.labels_)\n",
    "    db_n_clusters = len(list(labels.value_counts().values))\n",
    "    \n",
    "    # Chinese Whispers\n",
    "    log['randID'] = np.random.randint(100000, 999999, log.shape[0]).astype(str)\n",
    "    log['randID'] = log['id'] + log['randID']\n",
    "    d = log.drop(columns='id').set_index('randID').T.to_dict('list')\n",
    "    G = get_encoding_graph(d.items(), threshold=threshold)\n",
    "    # Do Chinese Whispers - Timed\n",
    "    cw_start = time.perf_counter()\n",
    "    cw_clusters = do_chinese_whispers(G)\n",
    "    cw_end = time.perf_counter()\n",
    "    cw_n_clusters = len(cw_clusters.keys())\n",
    "\n",
    "    # Print results\n",
    "    db_est_err = estimation_error(n_clusters=db_n_clusters, n_people=n_people)\n",
    "    cw_est_err = estimation_error(n_clusters=cw_n_clusters, n_people=n_people)\n",
    "    \n",
    "    print('DBScan: people: %d clusters: %d error: %f' % (n_people, db_n_clusters, db_est_err))\n",
    "    print('CW:     people: %d clusters: %d error: %f' % (n_people, cw_n_clusters, cw_est_err))\n",
    "\n",
    "    print(f\"DBscan took {db_end - db_start:0.4f} seconds\")\n",
    "    print(f\"CW     took {cw_end - cw_start:0.4f} seconds\") \n",
    "    print('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enormous-hometown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 24517 images of 6743 different people \n",
      "\n",
      "Log of length 10000 with 500 unique faces\n",
      "DBScan: people: 500 clusters: 451 error: 9.800000\n",
      "CW:     people: 500 clusters: 601 error: 20.200000\n",
      "DBscan took 1.8498 seconds\n",
      "CW     took 2.9047 seconds\n",
      "----------------------------------------------\n",
      "Log of length 10000 with 1000 unique faces\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7591b4241ae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'randID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'randID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'randID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'list'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_encoding_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Do Chinese Whispers - Timed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mcw_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Univeristy\\diss\\AIaaS\\face\\notebooks\\GenerateLogs.ipynb\u001b[0m in \u001b[0;36mget_encoding_graph\u001b[1;34m(encoding_list, threshold)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;34m\"    face_to_compare = np.array(face_to_compare)\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;34m\"    return np.sum(face_encodings*face_to_compare,axis=1)\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m     \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;34m\"def get_encoding_graph(encoding_list, threshold=0.75):\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;34m\"    \\\"\\\"\\\" Chinese Whispers Algorithm\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Univeristy\\diss\\AIaaS\\face\\notebooks\\GenerateLogs.ipynb\u001b[0m in \u001b[0;36mface_distance\u001b[1;34m(face_encodings, face_to_compare)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;34m\"    # pick from top people\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;34m\"    victim_id = random.choice(df['id'].value_counts().keys()[:25].to_list())\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;34m\"    # Resample the victim images until you have a df of size == log_length * proportion\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;34m\"    victim_images= df[df['id'] == victim_id].sample(n=int(log_length * proportion), \\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fix log_length, vary n_people\n",
    "log_length = 10000\n",
    "n_peoples = [500, 1000, 2500, 4000, 5000]\n",
    "\n",
    "epsilon = 9.8\n",
    "threshold = 72\n",
    "\n",
    "# Get data\n",
    "data_df = get_data(['lfw', 'cf'])\n",
    "\n",
    "for n_people in n_peoples:\n",
    "    # Generate log\n",
    "    log = generate_log(data_df, log_length=log_length, n_faces=n_people, exact=True).sample(frac=1).reset_index(drop=True)\n",
    "    n_people = len(log['id'].value_counts().keys())\n",
    "    print('Log of length %d with %d unique faces' % (len(log), n_people))\n",
    "        \n",
    "    # DBScan\n",
    "    X = log.drop(columns='id').values\n",
    "    # Do DBScan - Timed\n",
    "    db_start = time.perf_counter()\n",
    "    db = DBSCAN(eps=epsilon, min_samples=1).fit(X)\n",
    "    db_end = time.perf_counter()\n",
    "    labels = pd.Series(db.labels_)\n",
    "    db_n_clusters = len(list(labels.value_counts().values))\n",
    "    \n",
    "    # Chinese Whispers\n",
    "    log['randID'] = np.random.randint(100000, 999999, log.shape[0]).astype(str)\n",
    "    log['randID'] = log['id'] + log['randID']\n",
    "    d = log.drop(columns='id').set_index('randID').T.to_dict('list')\n",
    "    G = get_encoding_graph(d.items(), threshold=threshold)\n",
    "    # Do Chinese Whispers - Timed\n",
    "    cw_start = time.perf_counter()\n",
    "    cw_clusters = do_chinese_whispers(G)\n",
    "    cw_end = time.perf_counter()\n",
    "    cw_n_clusters = len(cw_clusters.keys())\n",
    "\n",
    "    # Print results\n",
    "    db_est_err = estimation_error(n_clusters=db_n_clusters, n_people=n_people)\n",
    "    cw_est_err = estimation_error(n_clusters=cw_n_clusters, n_people=n_people)\n",
    "    \n",
    "    print('DBScan: people: %d clusters: %d error: %f' % (n_people, db_n_clusters, db_est_err))\n",
    "    print('CW:     people: %d clusters: %d error: %f' % (n_people, cw_n_clusters, cw_est_err))\n",
    "\n",
    "    print(f\"DBscan took {db_end - db_start:0.4f} seconds\")\n",
    "    print(f\"CW     took {cw_end - cw_start:0.4f} seconds\") \n",
    "    print('----------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
