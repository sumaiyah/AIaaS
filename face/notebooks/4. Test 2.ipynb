{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conventional-double",
   "metadata": {},
   "source": [
    "# Stalker detection\n",
    "Given a dataset where specifc percentages of the data are one specific individual, can the algorithm correctly pick it out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revolutionary-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ipynb.fs.full.GenerateLogs import get_encoding_graph, generate_biased_log, get_data, do_chinese_whispers, estimation_error, generate_log_equal_proportion_each_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-flooring",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df = get_data(['lfw', 'cf'])\n",
    "epsilon = 9.8\n",
    "threshold = 72\n",
    "\n",
    "for length in [100, 1000, 2500, 5000, 10000]:\n",
    "    log = generate_biased_log(data_df, log_length=length, proportion=0.1)\n",
    "    n_people = len(log['id'].value_counts().keys())\n",
    "\n",
    "    # DBScan\n",
    "    X = log.drop(columns='id').values\n",
    "    db_start = time.perf_counter()\n",
    "    db = DBSCAN(eps=epsilon, min_samples=1).fit(X)\n",
    "    db_end = time.perf_counter()\n",
    "    labels = pd.Series(db.labels_)\n",
    "    db_n_clusters = len(list(labels.value_counts().values))\n",
    "    \n",
    "    # Chinese Whispers\n",
    "    log['randID'] = np.random.randint(100000, 999999, log.shape[0]).astype(str)\n",
    "    log['randID'] = log['id'] + log['randID']\n",
    "    d = log.drop(columns='id').set_index('randID').T.to_dict('list')\n",
    "    G = get_encoding_graph(d.items(), threshold=threshold)\n",
    "    # Do Chinese Whispers - Timed\n",
    "    cw_start = time.perf_counter()\n",
    "    cw_clusters = do_chinese_whispers(G)\n",
    "    cw_end = time.perf_counter()\n",
    "    cw_n_clusters = len(cw_clusters.keys())\n",
    " \n",
    "    print('true: ', log['id'].value_counts().values[:10].tolist())\n",
    "    print('db:   ', labels.value_counts().values[:10].tolist())\n",
    "    print('cw:   ', [len(cw_clusters[k]) for k in (sorted(cw_clusters, key=lambda k: len(cw_clusters[k]), reverse=True)[:10])])\n",
    "    \n",
    "    print(f\"DBscan took {db_end - db_start:0.4f} seconds\")\n",
    "    print(f\"CW     took {cw_end - cw_start:0.4f} seconds\")\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log of length 100 with 87 unique people and 10 images of one person\n",
    "# true:  [10, 2, 2, 2, 2, 1, 1, 1, 1, 1]\n",
    "# db:    [10, 3, 2, 2, 2, 1, 1, 1, 1, 1]\n",
    "# cw:    [10, 2, 2, 2, 2, 2, 1, 1, 1, 1]\n",
    "# DBscan took 0.0031 seconds\n",
    "# CW     took 0.0010 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 1000 with 711 unique people and 102 images of one person\n",
    "# true:  [102, 15, 5, 4, 4, 4, 4, 4, 3, 3]\n",
    "# db:    [205, 15, 5, 4, 4, 4, 4, 4, 3, 3]\n",
    "# cw:    [104, 16, 10, 8, 6, 5, 5, 5, 5, 4]\n",
    "# DBscan took 0.0279 seconds\n",
    "# CW     took 0.1246 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 2500 with 1413 unique people and 252 images of one person\n",
    "# true:  [252, 52, 22, 19, 14, 13, 10, 10, 9, 9]\n",
    "# db:    [667, 65, 22, 18, 11, 11, 9, 9, 8, 7]\n",
    "# cw:    [276, 67, 28, 19, 19, 19, 18, 17, 16, 15]\n",
    "# DBscan took 0.1147 seconds\n",
    "# CW     took 0.7799 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 5000 with 2281 unique people and 505 images of one person\n",
    "# true:  [505, 92, 47, 28, 21, 17, 16, 13, 12, 12]\n",
    "# db:    [1597, 94, 47, 28, 21, 19, 18, 13, 12, 11]\n",
    "# cw:    [522, 136, 58, 57, 57, 37, 36, 34, 27, 26]\n",
    "# DBscan took 0.4573 seconds\n",
    "# CW     took 4.3620 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 10000 with 3340 unique people and 1053 images of one person\n",
    "# true:  [1053, 217, 86, 34, 31, 28, 27, 26, 26, 25]\n",
    "# db:    [4563, 46, 36, 31, 24, 22, 21, 20, 20, 19]\n",
    "# cw:    [1194, 294, 108, 87, 83, 74, 68, 64, 59, 57]\n",
    "# DBscan took 1.8957 seconds\n",
    "# CW     took 30.4682 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-hunter",
   "metadata": {},
   "source": [
    "## Explore diff between dbscan and cw when clsuter sizes are all fairly large and equal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "departmental-willow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 24517 images of 6743 different people \n",
      "\n",
      "Log of length 10000 with 10 unique people and approx 1000 images of each person [1041, 1018, 1015, 1011, 1009, 1001, 1000, 972, 969, 964]\n",
      "true:  [1041, 1018, 1015, 1011, 1009, 1001, 1000, 972, 969, 964]\n",
      "db:    [2027, 1992, 1964, 1015, 999, 989, 972, 40, 2]\n",
      "cw:    [1018, 1015, 1009, 1000, 999, 989, 964, 958, 957, 951]\n",
      "DBscan took 2.0477 seconds\n",
      "CW     took 67.9384 seconds\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_df = get_data(['lfw', 'cf'])\n",
    "epsilon = 9.8\n",
    "threshold = 72\n",
    "#100, 1000, 2500, 5000\n",
    "for length in [10000]:\n",
    "    log = generate_log_equal_proportion_each_person(data_df, log_length=length, proportion=0.1)\n",
    "    n_people = len(log['id'].value_counts().keys())\n",
    "\n",
    "    # DBScan\n",
    "    X = log.drop(columns='id').values\n",
    "    db_start = time.perf_counter()\n",
    "    db = DBSCAN(eps=epsilon, min_samples=1).fit(X)\n",
    "    db_end = time.perf_counter()\n",
    "    labels = pd.Series(db.labels_)\n",
    "    db_n_clusters = len(list(labels.value_counts().values))\n",
    "    \n",
    "    # Chinese #Whispers\n",
    "    log['randID'] = np.random.randint(1, 99999999, log.shape[0]).astype(str)\n",
    "    log['randID'] = log['id'] + log['randID']\n",
    "    d = log.drop(columns='id').set_index('randID').T.to_dict('list')\n",
    "    G = get_encoding_graph(d.items(), threshold=threshold)\n",
    "    # Do Chinese Whispers - Timed\n",
    "    cw_start = time.perf_counter()\n",
    "    cw_clusters = do_chinese_whispers(G)\n",
    "    cw_end = time.perf_counter()\n",
    "    cw_n_clusters = len(cw_clusters.keys())\n",
    " \n",
    "    print('true: ', log['id'].value_counts().values[:10].tolist())\n",
    "    print('db:   ', labels.value_counts().values[:10].tolist())\n",
    "    print('cw:   ', [len(cw_clusters[k]) for k in (sorted(cw_clusters, key=lambda k: len(cw_clusters[k]), reverse=True)[:10])])\n",
    "    \n",
    "    print(f\"DBscan took {db_end - db_start:0.4f} seconds\")\n",
    "    print(f\"CW     took {cw_end - cw_start:0.4f} seconds\")\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contains 24517 images of 6743 different people \n",
    "\n",
    "# Log of length 100 with 10 unique people and approx 10 images of each person [14, 13, 11, 11, 11, 9, 8, 8, 8, 7]\n",
    "# true:  [14, 13, 11, 11, 11, 9, 8, 8, 8, 7]\n",
    "# db:    [14, 13, 11, 11, 11, 9, 8, 8, 7, 6]\n",
    "# cw:    [14, 13, 11, 11, 11, 9, 8, 8, 7, 7]\n",
    "# DBscan took 0.0019 seconds\n",
    "# CW     took 0.0071 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 1000 with 8 unique people and approx 100 images of each person [222, 191, 108, 101, 100, 95, 95, 88]\n",
    "# true:  [222, 191, 108, 101, 100, 95, 95, 88]\n",
    "# db:    [222, 183, 180, 108, 101, 99, 95, 6, 3, 2]\n",
    "# cw:    [222, 185, 108, 101, 100, 95, 79, 79, 6, 6]\n",
    "# DBscan took 0.0270 seconds\n",
    "# CW     took 0.8407 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 2500 with 10 unique people and approx 250 images of each person [273, 267, 261, 259, 249, 244, 240, 237, 236, 234]\n",
    "# true:  [273, 267, 261, 259, 249, 244, 240, 237, 236, 234]\n",
    "# db:    [514, 272, 264, 251, 240, 237, 234, 231, 211, 12]\n",
    "# cw:    [272, 264, 258, 251, 240, 237, 234, 234, 233, 206]\n",
    "# DBscan took 0.1247 seconds\n",
    "# CW     took 3.5773 seconds\n",
    "# ------------------------------------------------\n",
    "# Log of length 5000 with 10 unique people and approx 500 images of each person [558, 506, 498, 496, 496, 495, 495, 487, 486, 483]\n",
    "# true:  [558, 506, 498, 496, 496, 495, 495, 487, 486, 483]\n",
    "# db:    [536, 498, 496, 495, 486, 486, 484, 481, 478, 477]\n",
    "# cw:    [540, 498, 496, 495, 486, 484, 483, 483, 479, 478]\n",
    "# DBscan took 0.4493 seconds\n",
    "# CW     took 16.1882 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-commodity",
   "metadata": {},
   "source": [
    "## T-SNE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = generate_biased_log(data_df, log_length=600, proportion=0.10)\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(log.drop(columns='id').values)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "# unique colour for each person\n",
    "df = pd.DataFrame({'x':x, 'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(\n",
    "    x=\"x\", y=\"y\",\n",
    "    hue=log['id'].reset_index(drop=True),\n",
    "    palette=sns.color_palette(\"colorblind\", log['id'].nunique()),\n",
    "    data=df,\n",
    "    legend=False,\n",
    "    alpha=0.9,\n",
    "    s=60\n",
    ")\n",
    "\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.savefig('figs/test2-tsne-600.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
