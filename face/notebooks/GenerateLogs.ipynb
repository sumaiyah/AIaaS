{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordered-overhead",
   "metadata": {},
   "source": [
    "# Generate cloud computer vision logs\n",
    "\n",
    "**Input: Dataframe of embeddings that has an 'id' column**\n",
    "\n",
    "**Output: Log of a specific size with a specific number of unique faces**\n",
    "\n",
    "----------\n",
    "misc\n",
    "- Used a Zipfian distribution to produce logs of various sizes from the encodings\n",
    "- There are repeated encodings in the logs\n",
    "- Each experiment was carried out 5x and the results averaged\n",
    "- We generate log files (usage traces) where each entry\n",
    "includes a vector encoding representing a face; i.e. the\n",
    "logs represented that generated as a byproduct of using\n",
    "an AIaaS face service. Encodings are computed using\n",
    "the TensorFlow implementation of FaceNet (Sandberg 2020).\n",
    "\n",
    "-  Two types of log are generated: fixed trace uses\n",
    "a specified number (5000) of different faces for each version of the log; while varied trace uses a Zipfian distribution to select a varied number of different faces for\n",
    "each log size. Some encodings in the log are repeated to\n",
    "generate the required number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applicable-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "related-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(datasets):\n",
    "    # Load Datasets\n",
    "    data_path = '../data/'\n",
    "\n",
    "    names_path = lambda dataset_name: '%s%s-names.txt' % (data_path, dataset_name)\n",
    "    embeddings_path = lambda dataset_name: '%s%s_embeddings.npz' % (data_path, dataset_name)\n",
    "\n",
    "    # Return ids of images in each dataset as a list\n",
    "    def retrive_ids(filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            ids = file.read().split()\n",
    "        return ids\n",
    "\n",
    "    data_dfs = []\n",
    "    for name in datasets:\n",
    "        data_df = pd.DataFrame(np.load(embeddings_path(name))['arr_0'])\n",
    "        data_df['id'] = retrive_ids(names_path(name))\n",
    "\n",
    "        data_dfs.append(data_df)\n",
    "\n",
    "    all_data_df = pd.concat(data_dfs)\n",
    "    all_data_df = all_data_df.sample(frac=1).reset_index(drop=True)\n",
    "    print('Dataset contains %s images of %s different people \\n' % (len(all_data_df), len(all_data_df['id'].value_counts().keys())))\n",
    "    \n",
    "    return all_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focal-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a subset of df where 'id' has exactly n unqiue values\n",
    "def get_subset_n_unique_ids(df, n):\n",
    "    random_ids = pd.Series(df['id'].unique()).sample(n)\n",
    "    return df[df['id'].isin(random_ids)]\n",
    "\n",
    "# Return log of right size\n",
    "def resize_and_maintain_distribution(df, n, exact):\n",
    "    \"\"\"\n",
    "        df: dataframe with id column whos size want to reduce\n",
    "        n:  specified log length\n",
    "        exact: You want exactly n faces, if false, get roughly n faces and save a lot of time\n",
    "        \n",
    "        if      len(df) > log legnth: Keep distribution the same, remove a random image from each id (if count > 1), until log length = n\n",
    "        else if len(df) < log length: Keep distribution the same, repeat a random image from each id, until log length = n\n",
    "        else    log already of correct length\n",
    "    \"\"\"\n",
    "    # Save a LOT of time\n",
    "    if not exact: \n",
    "        df = df.sample(n=n, replace=True)\n",
    "#     print('(Generating Log: len(df)=%d, log_length=%d...)' % (len(df), n), end='')\n",
    "    \n",
    "    while len(df) < n:\n",
    "        unique_ids = df['id'].unique()\n",
    "        for unique_id in unique_ids:\n",
    "            df_subset = df[df['id'] == unique_id]\n",
    "            \n",
    "            # Randomly repeat one row else skip\n",
    "            df = pd.concat([df, \n",
    "                            pd.DataFrame(df_subset.sample(n=1).values, columns=df.columns) ], ignore_index=True)\n",
    "            \n",
    "            # Break if reached desired log length\n",
    "            if len(df) == n:\n",
    "                break  \n",
    "    \n",
    "    while len(df) > n:\n",
    "        unique_ids = df['id'].unique()\n",
    "        for unique_id in unique_ids:\n",
    "            df_subset = df[df['id'] == unique_id]\n",
    "            \n",
    "            # If more than one row randomly drop one row else skip\n",
    "            if len(df_subset) > 1:\n",
    "                df = df.drop(labels=df_subset.sample(n=1).index, axis=0)\n",
    "            \n",
    "            # Break if reached desired log length\n",
    "            if len(df) == n:\n",
    "                break  \n",
    "    \n",
    "#     print('done')\n",
    "    return df\n",
    "\n",
    "def generate_log(df, log_length, n_faces, exact=False):\n",
    "    \"\"\"\n",
    "        df: dataframe where each column is a dimension in the embedding space,\n",
    "            each row a different image and the final column contains the id of the person in the image\n",
    "        log_length:    control the length of the log\n",
    "        n_faces:       control the number of unique individuals in each log\n",
    "        \n",
    "        returns: dataframe with the same structure as embeddings_df, may have some repeated images\n",
    "    \"\"\"\n",
    "    # Repeatedly randomly sample df until you have n_faces unique faces and a len(log) >= log_length\n",
    "    counter=1\n",
    "    log = get_subset_n_unique_ids(df, n=n_faces)\n",
    "\n",
    "    # Resize the log, mantaining the distribution of images of each person\n",
    "    log = resize_and_maintain_distribution(log, n=log_length, exact=exact)\n",
    "    \n",
    "    # Checks\n",
    "    assert len(log) == log_length, 'log length incorrect %d != %d' % (len(log), log_length)\n",
    "    \n",
    "    if exact:\n",
    "        assert log.id.nunique() == n_faces, 'Incorrect number of unique faces %d != %d' % (log.id.nunique(), n_faces)\n",
    "    else:\n",
    "        print(\"WARNING: Given n_people=%d, we have n_people=%d\" % (n_faces, log.id.nunique()))\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "naked-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biased_log(df, log_length, proportion):\n",
    "    \"\"\"\n",
    "        df: original df youre sampling from\n",
    "        log_length: specified log length\n",
    "        proportion: approximately make this proportion of the log be one person 0.1=10%\n",
    "    \"\"\"\n",
    "    # pick from top people\n",
    "    victim_id = random.choice(df['id'].value_counts().keys()[:25].to_list())\n",
    "\n",
    "    # Resample the victim images until you have a df of size == log_length * proportion\n",
    "    victim_images= df[df['id'] == victim_id].sample(n=int(log_length * proportion), \n",
    "                                                    replace=True)\n",
    "\n",
    "    # Extract remainder of the log normally\n",
    "    non_victim_images = df.sample(n=int(log_length * (1-proportion)), replace=True)\n",
    "    \n",
    "    log = pd.concat([victim_images, non_victim_images])\n",
    "    \n",
    "    print('Log of length %d with %d unique people and %d images of one person' % (len(log), len(log['id'].value_counts().keys()),\n",
    "                                                                                  len(log[log['id']==victim_id])))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ahead-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_equal_proportion_each_person(df, log_length, proportion):\n",
    "    \"\"\"\n",
    "        df: original df youre sampling from\n",
    "        log_length: specified log length\n",
    "        proportion: approximately make this proportion of the log for each person person 0.1=10%\n",
    "    \"\"\"\n",
    "    log_dfs = []\n",
    "    for _ in range(int(1/proportion)):\n",
    "        # Resample the one persons images until you have a df of size == log_length * proportion\n",
    "        person_id = random.choice(df['id'].value_counts().keys()[:40].to_list())\n",
    "#         person_id = random.choice(df['id'].value_counts().keys().to_list())\n",
    "        person_images= df[df['id'] == person_id].sample(n=int(log_length * proportion), \n",
    "                                                        replace=True)\n",
    "        log_dfs.append(person_images)\n",
    "    \n",
    "    log = pd.concat(log_dfs)\n",
    "    \n",
    "    # Get correct log length\n",
    "    log = resize_and_maintain_distribution(log, log_length, exact=False)\n",
    "    \n",
    "    print('Log of length %d with %d unique people and approx %d images of each person %s' % (len(log), len(log['id'].value_counts().keys()),\n",
    "                                                                                  int(log_length * proportion), log['id'].value_counts().values.tolist()))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "molecular-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "from chinese_whispers import chinese_whispers, aggregate_clusters\n",
    "\n",
    "def face_distance(face_encodings, face_to_compare):\n",
    "    \"\"\"\n",
    "    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n",
    "    for each comparison face. The distance tells you how similar the faces are.\n",
    "    :param faces: List of face encodings to compare\n",
    "    :param face_to_compare: A face encoding to compare against\n",
    "    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    if len(face_encodings) == 0:\n",
    "        return np.empty((0))\n",
    "\n",
    "    #return 1/np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
    "    face_to_compare = np.array(face_to_compare)\n",
    "    return np.sum(face_encodings*face_to_compare,axis=1)\n",
    "\n",
    "def get_encoding_graph(encoding_list, threshold=0.75):\n",
    "    \"\"\" Chinese Whispers Algorithm\n",
    "    Modified from Alex Loveless' implementation,\n",
    "    http://alexloveless.co.uk/data/chinese-whispers-graph-clustering-in-python/\n",
    "    Inputs:\n",
    "        encoding_list: a list of facial encodings from face_recognition\n",
    "        threshold: facial match threshold,default 0.6\n",
    "        iterations: since chinese whispers is an iterative algorithm, number of times to iterate\n",
    "    Outputs:\n",
    "        sorted_clusters: a list of clusters, a cluster being a list of imagepaths,\n",
    "            sorted by largest cluster to smallest\n",
    "    \"\"\"\n",
    "\n",
    "    #from face_recognition.api import _face_distance\n",
    "    from random import shuffle\n",
    "    import networkx as nx\n",
    "    # Create graph\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    image_paths, encodings = zip(*encoding_list)\n",
    "\n",
    "    if len(encodings) <= 1:\n",
    "        print (\"No enough encodings to cluster!\")\n",
    "        return []\n",
    "\n",
    "    for idx, face_encoding_to_check in enumerate(encodings):\n",
    "        # Adding node of facial encoding\n",
    "        node_id = idx+1\n",
    "\n",
    "        # Initialize 'cluster' to unique value (cluster of itself)\n",
    "        node = (node_id, {'cluster': image_paths[idx], 'path': image_paths[idx]})\n",
    "        nodes.append(node)\n",
    "\n",
    "        # Facial encodings to compare\n",
    "        if (idx+1) >= len(encodings):\n",
    "            # Node is last element, don't create edge\n",
    "            break\n",
    "\n",
    "        compare_encodings = encodings[idx+1:]\n",
    "        distances = face_distance(compare_encodings, face_encoding_to_check)\n",
    "        encoding_edges = []\n",
    "        for i, distance in enumerate(distances):\n",
    "            if distance > threshold:\n",
    "                # Add edge if facial match\n",
    "                edge_id = idx+i+2\n",
    "                encoding_edges.append((node_id, edge_id, {'weight': distance}))\n",
    "\n",
    "        edges = edges + encoding_edges\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def do_chinese_whispers(G):\n",
    "    chinese_whispers(G, weighting='top', iterations=20)\n",
    "    \n",
    "    return aggregate_clusters(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "usual-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_error(n_clusters, n_people):\n",
    "    return (abs(n_clusters - n_people) / n_people) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
