{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "peaceful-original",
   "metadata": {},
   "source": [
    "# Preprocess Datasets\n",
    "\n",
    "Datasets:\n",
    "- Davison et al\n",
    "- Storm front\n",
    "- ...\n",
    "\n",
    "Preprocessing:\n",
    "- etc\n",
    "- Make binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smoking-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "raw_data_path = '../data/raw/'\n",
    "processed_data_path = '../data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrapped-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Removal of punctuation and capitlization\n",
    "## 2. Tokenizing\n",
    "## 3. Removal of stopwords\n",
    "## 4. Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "    \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # remove whitespace with a single space\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ')\n",
    "    # remove leading and trailing whitespace\n",
    "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','')\n",
    "    # replace normal numbers with numbr\n",
    "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    # removal of capitalization\n",
    "    tweet_lower = newtweet.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "horizontal-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre process Davidson dataset \n",
    "dataset = 'davison.csv'\n",
    "df = pd.read_csv(raw_data_path + dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-asset",
   "metadata": {},
   "source": [
    "**Class Label:** \n",
    "- 0 - hate speech \n",
    "- 1 - offensive language \n",
    "- 2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "photographic-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess tweets\n",
    "processed_tweets = preprocess(df.tweet)   \n",
    "df['processed_tweet'] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "honey-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hate_speech']        = df['hate_speech'] >= (df['count']/2)\n",
    "df['offensive_language'] = df['offensive_language'] >= (df['count']/2)\n",
    "df['neither']            = df['neither'] >= (df['count']/2)\n",
    "\n",
    "# only rows where only one of three classes = true\n",
    "df = df[(df['hate_speech'].astype(int) + df['offensive_language'].astype(int) + df['neither'].astype(int)) == 1]\n",
    "\n",
    "# # Binarize class 0,1 -> 1 and 2 -> 0\n",
    "df['class'] = df['class'].map({0:1, 1:1, 2:2})\n",
    "df['class'] = df['class'].map({1:1, 2:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "simple-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only tweet and class\n",
    "df[['tweet', 'processed_tweet', 'class']].to_csv(processed_data_path + dataset, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-crash",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fancy-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'founta.csv'\n",
    "df = pd.read_csv(raw_data_path + dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-paste",
   "metadata": {},
   "source": [
    "**Class Label:** \n",
    "- normal\n",
    "- spam\n",
    "- abusive\n",
    "- hateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "looking-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>maj_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849667487180259329</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>850490912954351616</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848791766853668864</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848306464892604416</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850010509969465344</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>847640895956459521</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>850335795022102530</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>848201162717265920</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>848939985160077312</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>847673586361843713</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id maj_label\n",
       "0      849667487180259329   abusive\n",
       "1      850490912954351616   abusive\n",
       "2      848791766853668864   abusive\n",
       "3      848306464892604416   abusive\n",
       "4      850010509969465344    normal\n",
       "...                   ...       ...\n",
       "79995  847640895956459521    normal\n",
       "79996  850335795022102530    normal\n",
       "79997  848201162717265920    normal\n",
       "79998  848939985160077312      spam\n",
       "79999  847673586361843713    normal\n",
       "\n",
       "[80000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-effectiveness",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "underlying-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waseem & Hovy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-greene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-novelty",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stormfront"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
